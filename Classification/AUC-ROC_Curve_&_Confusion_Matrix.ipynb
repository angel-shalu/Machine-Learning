{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13f0bec-9550-482d-a66a-fa4318ba7ea2",
   "metadata": {},
   "source": [
    " **AUC-ROC Curve & Confusion Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8f8f6-dbfd-4100-9119-548014003b59",
   "metadata": {},
   "source": [
    "# **What is a Confusion Matrix?**\n",
    "\n",
    "A **Confusion Matrix** is used to evaluate the performance of a classification model.\n",
    "\n",
    "It gives us a breakdown of the model's predictions:\n",
    "\n",
    "| Actual \\ Predicted | Predicted Negative (0) | Predicted Positive (1) |\n",
    "|--------------------|-------------------------|-------------------------|\n",
    "| **Actual Negative (0)** | True Negative (TN)         | False Positive (FP)        |\n",
    "| **Actual Positive (1)** | False Negative (FN)        | True Positive (TP)         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014e189-fee4-4248-ab74-3f77a70b5072",
   "metadata": {},
   "source": [
    "# **Real-Life Analogy – COVID Test**\n",
    "\n",
    "- **True Positive (TP)** → Sick person correctly predicted as sick   \n",
    "- **True Negative (TN)** → Healthy person correctly predicted as healthy   \n",
    "- **False Positive (FP)** → Healthy person predicted as sick  (Unnecessary panic)  \n",
    "- **False Negative (FN)** → Sick person predicted as healthy  (Very dangerous)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1344f4-3404-44ee-9f2e-b542aaa6ee0e",
   "metadata": {},
   "source": [
    "# **Model Evaluation Metrics**\n",
    "\n",
    "Here are four major metrics derived from the Confusion Matrix:\n",
    "\n",
    "| Metric      | Formula                              | Meaning |\n",
    "|-------------|--------------------------------------|---------|\n",
    "| **Accuracy**   | (TP + TN) / Total                     | Overall how many predictions were correct |\n",
    "| **Precision**  | TP / (TP + FP)                        | Of all predicted positives, how many were actually positive |\n",
    "| **Recall**     | TP / (TP + FN)                        | Of all actual positives, how many did the model correctly identify |\n",
    "| **F1 Score**   | 2 × (Precision × Recall) / (Precision + Recall) | Balance between Precision and Recall |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c666338-0261-46e9-961c-8076efc1b8ae",
   "metadata": {},
   "source": [
    "# What is Recall?\n",
    "\n",
    "**Recall** is also called **Sensitivity** or **True Positive Rate**.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "Recall = TP / (TP + FN)\n",
    "$$\n",
    "\n",
    "## Example:\n",
    "\n",
    "Let’s say we are building a cancer detection model. If 100 people actually have cancer and the model only catches 80 of them:\n",
    "\n",
    "- TP = 80 (correctly predicted positive)\n",
    "- FN = 20 (missed positive cases)\n",
    "\n",
    "Then:\n",
    "$$\n",
    "Recall = 80 / (80 + 20) = 0.80 or 80%\n",
    "$$\n",
    "\n",
    "## High Recall:\n",
    "- Catches most positive cases (great for **medical tests** or **fraud detection**)\n",
    "- May result in more false alarms (higher FP)\n",
    "\n",
    "##  Low Recall:\n",
    "- Misses many positive cases — risky in sensitive applications!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0075a4-a84d-4fa3-980c-4a1b9f7ade9c",
   "metadata": {},
   "source": [
    "# **ROC Curve and AUC – Understanding Performance**\n",
    "\n",
    "- **ROC (Receiver Operating Characteristic)** curve plots:\n",
    "  - X-axis → **False Positive Rate (FPR)**\n",
    "  - Y-axis → **True Positive Rate (TPR or Recall)**\n",
    "- **AUC (Area Under Curve)** measures the area under the ROC curve  \n",
    "  - AUC close to **1.0** → Excellent model  \n",
    "  - AUC around **0.5** → Random guess (bad model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d86353-d409-487e-b2a8-40a21528036f",
   "metadata": {},
   "source": [
    "# **How to Read the ROC Curve**\n",
    "\n",
    "| AUC Score | Model Interpretation           |\n",
    "|-----------|--------------------------------|\n",
    "| 1.0       | Perfect classifier (Best)      |\n",
    "| 0.9+      | Excellent model                |\n",
    "| 0.7–0.8   | Good / Acceptable              |\n",
    "| 0.5       | No discrimination (Random)     |\n",
    "| < 0.5     | Worse than random              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9b4b0-baeb-4df6-b161-927ad470567e",
   "metadata": {},
   "source": [
    "# Experiment Results – Scaling Comparison\n",
    "\n",
    "## Best Case – Using `StandardScaler`\n",
    "\n",
    "- Clean diagonal confusion matrix (correct predictions)\n",
    "- ROC curve close to top-left (high recall & low FPR)\n",
    "- **AUC: &&High (≈0.90+)&&**\n",
    "- **Accuracy: $$~90%+$$**\n",
    "\n",
    "![Best Model – StandardScaler](Day51_StandardScaler_Confusion_ROC.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2e124-26e4-4691-96eb-72f05ed6df47",
   "metadata": {},
   "source": [
    "## Worst Case – Using `Normalizer`\n",
    "\n",
    "- More off-diagonal elements (wrong predictions)\n",
    "- ROC curve flatter to less distinction between classes\n",
    "- **AUC: Lower $$(≈0.6–0.7)$$**\n",
    "- **Accuracy: Drops significantly**\n",
    "\n",
    "![Worst Model – Normalizer](Day51_Normalizer_Confusion_ROC.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a119d-8f36-4589-a53a-bf0916333fb2",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Use **Confusion Matrix** to count correct vs incorrect predictions  \n",
    "Use **Recall** when missing actual positives is dangerous  \n",
    "Use **ROC Curve & AUC** to evaluate models across thresholds  \n",
    "**StandardScaler** performs better than **Normalizer** for Logistic Regression  \n",
    "Test multiple preprocessing methods to optimize performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
