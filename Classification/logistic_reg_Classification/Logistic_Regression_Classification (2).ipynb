{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1884f6-93b6-4db5-8266-01d9d5607e83",
   "metadata": {},
   "source": [
    "**Logistic Regression** \n",
    "\n",
    "Welcome!\n",
    "\n",
    "Today, we're going to learn one of the most important and widely used classification algorithms in machine learning — **Logistic Regression**.\n",
    "\n",
    "While the name includes “regression,” don’t let it confuse you — logistic regression is actually used for **classification problems**.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "- What is logistic regression and why it's used\n",
    "- Difference between classification and regression\n",
    "- Key evaluation metrics like **confusion matrix**, **accuracy**, **precision**, **recall**, and **F1 score**\n",
    "- Implementing logistic regression in Python step-by-step (without using loops/functions)\n",
    "- Comparing performance using different preprocessing techniques like **StandardScaler** and **Normalizer**\n",
    "- Checking **bias** and **variance**\n",
    "- Ending with a summary and real-world recommendations\n",
    "\n",
    "Let’s get started and understand how logistic regression helps in **predicting categories** like:\n",
    "- Will a customer buy or not?\n",
    "- Is an email spam or not?\n",
    "- Will a student pass or fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ac506-d761-4c5c-85c5-78d6bf1c0b48",
   "metadata": {},
   "source": [
    "# Introduction to Classification\n",
    "\n",
    "- Classification is used when the **dependent variable is binary** (e.g., Yes/No, 1/0).\n",
    "- Unlike regression (which predicts continuous values), classification predicts discrete categories.\n",
    "- The performance of **classification** is evaluated using the **confusion matrix**, not\n",
    "- The performance of **Regression** is evaluated using the **R² or Adjusted R²**.\n",
    "\n",
    "## Steps to Build Classification Model:\n",
    "\n",
    "1. Split data into x_train, x_test, y_train, y_test\n",
    "2. Train model on x_train and y_train\n",
    "3. Predict y_pred using x_test\n",
    "4. Evaluate using y_test vs y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128b8a1-2c76-4b1e-ac31-4c9e45294dd5",
   "metadata": {},
   "source": [
    "# Understanding Confusion Matrix\n",
    "\n",
    "- Confusion Matrix helps compare actual labels vs predicted labels:\n",
    "  - TP (True Positive)\n",
    "  - TN (True Negative)\n",
    "  - FP (False Positive)\n",
    "  - FN (False Negative)\n",
    "\n",
    "**Example**: Diagnosing COVID  \n",
    "\n",
    "Actual (Patient) vs Predicted (Doctor)\n",
    "- Patient no COVID, Doctor says no COVID: TN\n",
    "- Patient no COVID, Doctor says yes COVID: FP\n",
    "- Patient yes COVID, Doctor says no COVID: FN\n",
    "- Patient yes COVID, Doctor says yes COVID: TP\n",
    "\n",
    "\n",
    "| Actual \\ Predicted | No COVID (0) | Yes COVID (1) |\n",
    "|--------------------|--------------|---------------|\n",
    "| No COVID (0)       | TN           | FP            |\n",
    "| Yes COVID (1)      | FN           | TP            |\n",
    "\n",
    "## Key Metrics:\n",
    "- Accuracy = (TP + TN) / Total\n",
    "- Error Rate = (FP + FN) / Total\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN)\n",
    "- F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Sometimes TN and TP can flip depending on interpretation. Always define clearly.\n",
    "\n",
    "Is Logistic Regression a classification algorithm? (Short)\n",
    "- YES\n",
    "- It uses a regression line to separate two classes\n",
    "- Applies a sigmoid function to model probabilities\n",
    "- Based on threshold (like 0.5), it classifies outputs\n",
    "\n",
    "Also used in deep learning as Sigmoid Activation\n",
    "\n",
    "Logistic Regression = MaxEnt Classifier\n",
    "\n",
    "- y * mx > 0 => Correct classification\n",
    "- y * mx < 0 => Misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09edc0-761a-4210-9fcd-e50d982e1cfd",
   "metadata": {},
   "source": [
    "**Is Logistic Regression a Classification Algorithm?**\n",
    "    \n",
    "Yes, Logistic Regression **is** a classification algorithm. Here's the explanation:\n",
    "\n",
    "1. **Despite the name**, Logistic Regression is **not** used for regression problems (predicting continuous values). It's used when the output variable is **categorical**, typically binary (0 or 1).\n",
    "\n",
    "2. Logistic Regression works by drawing a **regression line** that separates two classes.\n",
    "\n",
    "3. It then applies a **sigmoid function** (also called the logistic function) to convert the linear output into a probability between 0 and 1.\n",
    "\n",
    "4. If the output probability is **greater than 0.5**, the model predicts class 1. If it's **less than 0.5**, it predicts class 0. You can change this threshold if needed.\n",
    "\n",
    "5. This is why it’s considered a **probability-based classification model**.\n",
    "\n",
    "6. Logistic Regression is also used in **deep learning** under the name **sigmoid activation function**.\n",
    "\n",
    "7. The algorithm is also known as the **Maximum Entropy (MaxEnt) Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8aea4d-51b8-43c7-8772-8d1a6bb66529",
   "metadata": {},
   "source": [
    "# What is Logistic Regression?\n",
    "Logistic Regression:\n",
    "\n",
    "- Though the name has \"regression\", it's a **classification** algorithm.\n",
    "- It models the probability that a given input belongs to a particular class.\n",
    "- Logistic regression uses a **sigmoid (S-shaped)** curve to separate two classes (0 and 1).\n",
    "- It works well for binary classification (like \"Purchased\" or \"Not Purchased\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388525a2-0d4b-4aac-be99-6ff68d684ca1",
   "metadata": {},
   "source": [
    "#  Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59075914-a1f4-40da-a995-081c768a30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d5d68-a04d-4410-a9d7-eaf165116463",
   "metadata": {},
   "source": [
    "# Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52563086-f2fa-41dc-bf1b-64596a6294d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    User ID  Gender  Age  EstimatedSalary  Purchased\n",
      "0  15624510    Male   19            19000          0\n",
      "1  15810944    Male   35            20000          0\n",
      "2  15668575  Female   26            43000          0\n",
      "3  15603246  Female   27            57000          0\n",
      "4  15804002    Male   19            76000          0\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r\"C:\\Users\\Lenovo\\Downloads\\logit classification.csv\") # Change path if needed\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6275bb2-ec13-493a-903f-59dee06ebc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID            0\n",
      "Gender             0\n",
      "Age                0\n",
      "EstimatedSalary    0\n",
      "Purchased          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cddb19-00f1-4439-bebf-40766f609c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525b8c1-44e7-439c-9f10-69316675f3c3",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a946435a-7e10-48a2-b487-9646dd230775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "X = dataset.iloc[:, [2, 3]].values  # Age and EstimatedSalary\n",
    "y = dataset.iloc[:, -1].values      # Purchased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a360a1-3edd-44dc-afea-24b700711694",
   "metadata": {},
   "source": [
    "# Try 1: No Scaling, random_state = 0, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c51b83-563f-43f5-a6c3-dfc90abe73b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[65  3]\n",
      " [ 8 24]]\n",
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Try 1: No Scaling, random_state = 0, test_size = 0.25\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train1, y_train1)\n",
    "y_pred1 = model1.predict(X_test1)\n",
    "cm1 = confusion_matrix(y_test1, y_pred1)\n",
    "acc1 = accuracy_score(y_test1, y_pred1)\n",
    "print(\"Confusion Matrix:\\n\", cm1)\n",
    "print(\"Accuracy:\", acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609003f-4095-4d1a-97ef-33171e1a6119",
   "metadata": {},
   "source": [
    "## Try 2: StandardScaler, random_state = 0, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce13bded-edf5-4627-9300-345e882e734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[65  3]\n",
      " [ 8 24]]\n",
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Try 2: StandardScaler, random_state = 0, test_size = 0.25\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "X_train2 = scaler2.fit_transform(X_train2)\n",
    "X_test2 = scaler2.transform(X_test2)\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train2, y_train2)\n",
    "y_pred2 = model2.predict(X_test2)\n",
    "cm2 = confusion_matrix(y_test2, y_pred2)\n",
    "acc2 = accuracy_score(y_test2, y_pred2)\n",
    "print(\"Confusion Matrix:\\n\", cm2)\n",
    "print(\"Accuracy:\", acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd6229-e44a-49ab-892d-0450bf02a079",
   "metadata": {},
   "source": [
    "## Try 3: Normalizer, random_state = 0, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950bac97-20d8-4fa7-acd5-33f18e07bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[68  0]\n",
      " [32  0]]\n",
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Try 3: Normalizer, random_state = 0, test_size = 0.25\n",
    "\n",
    "norm3 = Normalizer()\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "X_train3 = norm3.fit_transform(X_train3)\n",
    "X_test3 = norm3.transform(X_test3)\n",
    "model3 = LogisticRegression()\n",
    "model3.fit(X_train3, y_train3)\n",
    "y_pred3 = model3.predict(X_test3)\n",
    "cm3 = confusion_matrix(y_test3, y_pred3)\n",
    "acc3 = accuracy_score(y_test3, y_pred3)\n",
    "print(\"Confusion Matrix:\\n\", cm3)\n",
    "print(\"Accuracy:\", acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b40f4f-bce0-40ad-842a-96691a33fd74",
   "metadata": {},
   "source": [
    "## Try 4: StandardScaler, random_state = 100, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f60dd35-a720-47ff-a821-fde293cfa4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[62  3]\n",
      " [12 23]]\n",
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Try 4: StandardScaler, random_state = 100, test_size = 0.25\n",
    "\n",
    "scaler4 = StandardScaler()\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.25, random_state=100)\n",
    "X_train4 = scaler4.fit_transform(X_train4)\n",
    "X_test4 = scaler4.transform(X_test4)\n",
    "model4 = LogisticRegression()\n",
    "model4.fit(X_train4, y_train4)\n",
    "y_pred4 = model4.predict(X_test4)\n",
    "cm4 = confusion_matrix(y_test4, y_pred4)\n",
    "acc4 = accuracy_score(y_test4, y_pred4)\n",
    "print(\"Confusion Matrix:\\n\", cm4)\n",
    "print(\"Accuracy:\", acc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0f2b0-0a9f-4c8d-a0bf-195a54abfa3b",
   "metadata": {},
   "source": [
    "# Try 5: StandardScaler, random_state = 51, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5fbe67a-99fe-4ac1-98a8-d569e869304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[61  4]\n",
      " [ 9 26]]\n",
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "#Try 5: StandardScaler, random_state = 51, test_size = 0.25\n",
    "\n",
    "scaler5 = StandardScaler()\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.25, random_state=51)\n",
    "X_train5 = scaler5.fit_transform(X_train5)\n",
    "X_test5 = scaler5.transform(X_test5)\n",
    "model5 = LogisticRegression()\n",
    "model5.fit(X_train5, y_train5)\n",
    "y_pred5 = model5.predict(X_test5)\n",
    "cm5 = confusion_matrix(y_test5, y_pred5)\n",
    "acc5 = accuracy_score(y_test5, y_pred5)\n",
    "print(\"Confusion Matrix:\\n\", cm5)\n",
    "print(\"Accuracy:\", acc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417fe80f-1589-4d00-b04b-ff4229f3f074",
   "metadata": {},
   "source": [
    "# Try 6: Function to run experiment for new test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c76b36-72d7-4ad3-8f54-767b3eca09bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 6: Function to run experiment for new test case\n",
    "\n",
    "def run_logistic_test(X, y, scaler, test_size, random_state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    if scaler:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with {scaler.__class__.__name__ if scaler else 'No Scaling'}, RS={random_state}, TS={test_size}: {acc:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd1a67f-6934-4ff8-bf00-150ead1fac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with StandardScaler, RS=21, TS=0.25: 0.8600\n",
      "Confusion Matrix:\n",
      " [[65  2]\n",
      " [12 21]]\n",
      "Accuracy with StandardScaler, RS=41, TS=0.25: 0.8300\n",
      "Confusion Matrix:\n",
      " [[59  4]\n",
      " [13 24]]\n",
      "Accuracy with StandardScaler, RS=11, TS=0.25: 0.8300\n",
      "Confusion Matrix:\n",
      " [[63  3]\n",
      " [14 20]]\n",
      "Accuracy with StandardScaler, RS=2, TS=0.25: 0.8100\n",
      "Confusion Matrix:\n",
      " [[56  6]\n",
      " [13 25]]\n"
     ]
    }
   ],
   "source": [
    "acc6 = run_logistic_test(X, y, StandardScaler(), 0.25, 21)\n",
    "acc7 = run_logistic_test(X, y, StandardScaler(), 0.25, 41)\n",
    "acc8 = run_logistic_test(X, y, StandardScaler(), 0.25, 11)\n",
    "acc9 = run_logistic_test(X, y, StandardScaler(), 0.25, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "194a37b9-46e5-413c-a4f7-2bbb6a8c8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Accuracy Comparisons ---\n",
      "1. No Scaling, rs=0:        0.89\n",
      "2. StandardScaler, rs=0:    0.89\n",
      "3. Normalizer, rs=0:        0.68\n",
      "4. StandardScaler, rs=100:  0.85\n",
      "5. StandardScaler, rs=51:   0.87\n",
      "6. StandardScaler, rs=21:    0.86\n",
      "7. StandardScaler, rs=41:    0.83\n",
      "8. StandardScaler, rs=11:    0.83\n",
      "9. StandardScaler, rs=2:     0.81\n"
     ]
    }
   ],
   "source": [
    "# 8. Compare Accuracies\n",
    "print(\"\\n--- Accuracy Comparisons ---\")\n",
    "print(\"1. No Scaling, rs=0:       \", acc1)\n",
    "print(\"2. StandardScaler, rs=0:   \", acc2)\n",
    "print(\"3. Normalizer, rs=0:       \", acc3)\n",
    "print(\"4. StandardScaler, rs=100: \", acc4)\n",
    "print(\"5. StandardScaler, rs=51:  \", acc5)\n",
    "print(\"6. StandardScaler, rs=21:   \", acc6)\n",
    "print(\"7. StandardScaler, rs=41:   \", acc7)\n",
    "print(\"8. StandardScaler, rs=11:   \", acc8)\n",
    "print(\"9. StandardScaler, rs=2:    \", acc9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2108e5-22d3-46ce-b745-5382147a3d79",
   "metadata": {},
   "source": [
    "## Bias and Variance Check\n",
    "\n",
    "- Let's check how well the model performs on training and test data.\n",
    "- High training accuracy means low bias → model has learned training patterns well.\n",
    "- If test accuracy is close to training accuracy, variance is low → good generalization.\n",
    "- If there's a large gap, model might be overfitting (high variance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159cd334-071d-4605-b65b-c555a27efc5b",
   "metadata": {},
   "source": [
    "- You don’t need to check bias and variance for every model.\n",
    "- Just test it on key configurations:\n",
    "  - Best performing model\n",
    "  - Worst or unstable model (optional)\n",
    "- This helps identify if your model is underfitting (high bias) or overfitting (high variance).\n",
    "- Balanced train/test accuracy means good generalization.\n",
    "\n",
    "In our notebook, we observed that the best model using **`StandardScaler`** (RS=0, TestSize=0.2) had high training and test accuracy — confirming good balance and low variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371bc9d8-dfd6-44ed-a2d9-068aa2681e60",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We tested Logistic Regression using different preprocessing methods and random states.\n",
    "\n",
    "- Best overall accuracy: 0.89\n",
    "- Scaling data with StandardScaler provided better results than using raw or normalized data.\n",
    "- Normalizer performed worst, possibly due to the nature of the features (Age, Salary).\n",
    "- Bias and Variance scores were close, suggesting that the model generalizes well and is not overfitting.\n",
    "- \n",
    "In future, try regularization and cross-validation to further improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0ea46-8400-4007-8668-763144dcc40d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3f496-628b-496f-a065-c78230bf0904",
   "metadata": {},
   "source": [
    "# Using **Functions, loops** and **dictionaries** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d2b4e-25fe-487f-8d89-bd2ea823d109",
   "metadata": {},
   "source": [
    "The section above was written for beginners to understand each step manually.\n",
    "\n",
    "But in real-world or advanced use cases, we use **Functions, loops and dictionaries** to write cleaner, more scalable code.\n",
    "Below is an example that does the **same thing** as above but in a short and elegant way using a loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9298380f-de05-4cff-9fa1-2bbf6a17413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Test with different configurations\n",
    "results = []\n",
    "scalers = {\n",
    "    'No Scaling': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'Normalizer': Normalizer()\n",
    "}\n",
    "\n",
    "random_states = [100, 51, 21, 41, 11, 2, 0]\n",
    "test_sizes = [0.25, 0.2]\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    for rs in random_states:\n",
    "        for ts in test_sizes:\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=rs)\n",
    "            \n",
    "            # Apply scaling\n",
    "            if scaler:\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "            # Train model\n",
    "            classifier = LogisticRegression()\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "\n",
    "            # Metrics\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "            results.append({\n",
    "                'Scaler': scaler_name,\n",
    "                'RandomState': rs,\n",
    "                'TestSize': ts,\n",
    "                'Accuracy': round(acc, 4),\n",
    "                'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb4f03-6b66-4fde-b229-3f821b61c5d0",
   "metadata": {},
   "source": [
    "## Convert Results to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f22f46-6598-4a96-b230-07561b513d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert Results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df_sorted = results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54488c-68c6-4911-9e4c-f78a19513f78",
   "metadata": {},
   "source": [
    "## Display All Configurations and Best One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70caea0e-5c98-4ed1-8698-284c5baf6d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Configurations Tested:\n",
      "            Scaler  RandomState  TestSize  Accuracy  TP  TN  FP  FN\n",
      "0   StandardScaler            0      0.20    0.9250  17  57   1   5\n",
      "1       No Scaling            0      0.20    0.9125  17  56   2   5\n",
      "2       No Scaling            0      0.25    0.8900  24  65   3   8\n",
      "3   StandardScaler            0      0.25    0.8900  24  65   3   8\n",
      "4   StandardScaler           51      0.20    0.8875  20  51   1   8\n",
      "5       No Scaling           51      0.20    0.8750  20  50   2   8\n",
      "6   StandardScaler           51      0.25    0.8700  26  61   4   9\n",
      "7   StandardScaler           21      0.20    0.8625  15  54   2   9\n",
      "8       No Scaling           21      0.20    0.8625  15  54   2   9\n",
      "9       No Scaling           51      0.25    0.8600  26  60   5   9\n",
      "10      No Scaling           21      0.25    0.8600  21  65   2  12\n",
      "11  StandardScaler           21      0.25    0.8600  21  65   2  12\n",
      "12  StandardScaler          100      0.25    0.8500  23  62   3  12\n",
      "13      No Scaling          100      0.25    0.8500  23  62   3  12\n",
      "14      No Scaling           11      0.25    0.8300  20  63   3  14\n",
      "15  StandardScaler           41      0.25    0.8300  24  59   4  13\n",
      "16  StandardScaler           11      0.25    0.8300  20  63   3  14\n",
      "17      No Scaling          100      0.20    0.8250  20  46   3  11\n",
      "18  StandardScaler          100      0.20    0.8250  20  46   3  11\n",
      "19  StandardScaler           11      0.20    0.8250  16  50   3  11\n",
      "20      No Scaling           41      0.25    0.8200  24  58   5  13\n",
      "21      No Scaling           11      0.20    0.8125  16  49   4  11\n",
      "22  StandardScaler            2      0.20    0.8125  20  45   3  12\n",
      "23      No Scaling            2      0.20    0.8125  20  45   3  12\n",
      "24  StandardScaler            2      0.25    0.8100  25  56   6  13\n",
      "25      No Scaling            2      0.25    0.8100  25  56   6  13\n",
      "26  StandardScaler           41      0.20    0.7875  17  46   4  13\n",
      "27      No Scaling           41      0.20    0.7750  17  45   5  13\n",
      "28      Normalizer            0      0.20    0.7250   0  58   0  22\n",
      "29      Normalizer           21      0.20    0.7000   0  56   0  24\n",
      "30      Normalizer            0      0.25    0.6800   0  68   0  32\n",
      "31      Normalizer           21      0.25    0.6700   0  67   0  33\n",
      "32      Normalizer           11      0.20    0.6625   0  53   0  27\n",
      "33      Normalizer           11      0.25    0.6600   0  66   0  34\n",
      "34      Normalizer          100      0.25    0.6500   0  65   0  35\n",
      "35      Normalizer           51      0.25    0.6500   0  65   0  35\n",
      "36      Normalizer           51      0.20    0.6500   0  52   0  28\n",
      "37      Normalizer           41      0.25    0.6300   0  63   0  37\n",
      "38      Normalizer           41      0.20    0.6250   0  50   0  30\n",
      "39      Normalizer            2      0.25    0.6200   0  62   0  38\n",
      "40      Normalizer          100      0.20    0.6125   0  49   0  31\n",
      "41      Normalizer            2      0.20    0.6000   0  48   0  32\n",
      "\n",
      " Best Accuracy:\n",
      "Scaler         StandardScaler\n",
      "RandomState                 0\n",
      "TestSize                  0.2\n",
      "Accuracy                0.925\n",
      "TP                         17\n",
      "TN                         57\n",
      "FP                          1\n",
      "FN                          5\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display All Configurations and Best One\n",
    "print(\"\\nAll Configurations Tested:\")\n",
    "print(results_df_sorted)\n",
    "\n",
    "best = results_df_sorted.iloc[0]\n",
    "print(\"\\n Best Accuracy:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad5606c7-d50e-4d56-9e0a-d3a0b73b244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to CSV\n",
    "results_df_sorted.to_csv(\"logistic_regression_experiments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffb4e2-76fd-495f-9c1f-53e03c3f3d1b",
   "metadata": {},
   "source": [
    "# Final Results Table Summary\n",
    "\n",
    "After testing 42 combinations (scalers, test sizes, random states), here's what we found:\n",
    "\n",
    "-  **Best Accuracy**: **92.5%**\n",
    "-  Best achieved with:\n",
    "  - **Scaler**: `StandardScaler`\n",
    "  - **RandomState**: `0`\n",
    "  - **TestSize**: `0.2`\n",
    "\n",
    "## Detailed Best Result:\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Accuracy | 92.5% |\n",
    "| True Positives (TP) | 17 |\n",
    "| True Negatives (TN) | 57 |\n",
    "| False Positives (FP) | 1 |\n",
    "| False Negatives (FN) | 5 |\n",
    "\n",
    "## Interpretation:\n",
    "\n",
    "- This combination had the **highest accuracy** and **lowest error values**.\n",
    "- Very few wrong predictions:\n",
    "  - Only **1 FP** and **5 FN**\n",
    "- Shows that **StandardScaler preprocessing** works best for this dataset.\n",
    "- Normalizer consistently underperformed due to scale-sensitive features (like Age, Salary).\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "- Use `StandardScaler` when working with Logistic Regression and numeric features.\n",
    "- RandomState and TestSize also affect performance — it's a good practice to tune them.\n",
    "- This method helps you identify the **most reliable configuration** quickly and cleanly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
